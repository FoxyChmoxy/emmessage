{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "import random\n",
    "\n",
    "stopwords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.json') as config_file:\n",
    "    data = json.load(config_file)\n",
    "    \n",
    "wassa = data[\"WASSA\"]\n",
    "archive = data[\"archive\"]\n",
    "corporate = data[\"corporate\"]\n",
    "\n",
    "global_dataset = np.array([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WASSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clearify_post(post):\n",
    "    data = list(map(lambda x: x.rstrip(), post.split('\\t')))\n",
    "    return { \"text\": data[1], \"emotion\": data[2] }\n",
    "\n",
    "def get_wassa_dataset(url):\n",
    "    file = urllib.request.urlopen(url)\n",
    "    posts = list(map(lambda x : x.decode(\"utf-8\"), file.readlines()))\n",
    "    return list(map(clearify_post, posts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local dataset: 857\n",
      "local dataset: 84\n",
      "local dataset: 1147\n",
      "local dataset: 110\n",
      "local dataset: 823\n",
      "local dataset: 79\n",
      "local dataset: 786\n",
      "local dataset: 74\n",
      "total local dataset: 3960\n",
      "global dataset: 3960\n"
     ]
    }
   ],
   "source": [
    "total_local_dataset = np.array([])\n",
    "for key in wassa:\n",
    "    for dataset_version in wassa[key]:\n",
    "        local_dataset = get_wassa_dataset(wassa[key][dataset_version])\n",
    "        global_dataset = np.concatenate((global_dataset, np.array(local_dataset)))\n",
    "        total_local_dataset = np.concatenate((total_local_dataset, np.array(local_dataset)))\n",
    "        print(\"local dataset:\", len(local_dataset))\n",
    "print(\"total local dataset:\", total_local_dataset.shape[0])\n",
    "print(\"global dataset:\", global_dataset.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emotion</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>1257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         text\n",
       "emotion      \n",
       "anger     941\n",
       "fear     1257\n",
       "joy       902\n",
       "sadness   860"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(list(total_local_dataset))\n",
    "df[['emotion', 'text']].groupby(['emotion']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clearify_comment(comment):\n",
    "    clear_comment = comment.rstrip().split(';')\n",
    "    return { \"text\" : clear_comment[0], \"emotion\": clear_comment[1] }\n",
    "\n",
    "def get_archive_dataset(url):\n",
    "    with open(url) as file:\n",
    "        data = file.readlines()\n",
    "    return list(map(clearify_comment, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local dataset: 2000\n",
      "local dataset: 2000\n",
      "local dataset: 16000\n",
      "total local dataset: 20000\n",
      "global dataset: 23960\n"
     ]
    }
   ],
   "source": [
    "total_local_dataset = np.array([])\n",
    "for key in archive:\n",
    "    for dataset_version in archive[key]:\n",
    "        local_dataset = get_archive_dataset(archive[key][dataset_version])\n",
    "        total_local_dataset = np.concatenate((total_local_dataset, np.array(local_dataset)))\n",
    "        global_dataset = np.concatenate((global_dataset, np.array(local_dataset)))\n",
    "        print(\"local dataset:\", len(local_dataset))\n",
    "print(\"total local dataset:\", total_local_dataset.shape[0])\n",
    "print(\"global dataset:\", global_dataset.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emotion</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>2709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>2373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>6761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>1641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>5797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          text\n",
       "emotion       \n",
       "anger     2709\n",
       "fear      2373\n",
       "joy       6761\n",
       "love      1641\n",
       "sadness   5797\n",
       "surprise   719"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(list(total_local_dataset))\n",
    "df[['emotion', 'text']].groupby(['emotion']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local dataset: 3118\n",
      "total local dataset: 3118\n",
      "global dataset: 27078\n"
     ]
    }
   ],
   "source": [
    "def clearify_corporate(text):\n",
    "    return { \"text\" : text, \"emotion\" : \"corporate\" }\n",
    "\n",
    "total_local_dataset = np.array([])\n",
    "with open(corporate, encoding = \"ISO-8859-1\") as csvfile:\n",
    "    corporate_reader = csv.DictReader(csvfile, delimiter=',')\n",
    "    reviews = [row['text'] for row in corporate_reader]\n",
    "    local_dataset = list(map(clearify_corporate, reviews))\n",
    "    total_local_dataset = np.concatenate((total_local_dataset, np.array(local_dataset)))\n",
    "    global_dataset = np.concatenate((global_dataset, np.array(local_dataset)))\n",
    "    print(\"local dataset:\", len(local_dataset))\n",
    "print(\"total local dataset:\", total_local_dataset.shape[0])\n",
    "print(\"global dataset:\", global_dataset.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emotion</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>corporate</th>\n",
       "      <td>3118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           text\n",
       "emotion        \n",
       "corporate  3118"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(list(total_local_dataset))\n",
    "df[['emotion', 'text']].groupby(['emotion']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas & NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text       27078\n",
      "emotion    27078\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emotion</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>3650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>corporate</th>\n",
       "      <td>3118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>3630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>7663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>1641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>6657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           text\n",
       "emotion        \n",
       "anger      3650\n",
       "corporate  3118\n",
       "fear       3630\n",
       "joy        7663\n",
       "love       1641\n",
       "sadness    6657\n",
       "surprise    719"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list(np.array) wtf?\n",
    "df = pd.DataFrame(list(global_dataset))\n",
    "print(df.count())\n",
    "df[['emotion', 'text']].groupby(['emotion']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(tokens, stop_words = ()):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    cleaned_tokens = []\n",
    "    for token in tokens:\n",
    "        if len(token) > 0 and not re.search(r'[^0-9a-zA-Z]+', token) and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(stemmer.stem(token))\n",
    "    return cleaned_tokens\n",
    "\n",
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token\n",
    "\n",
    "def get_tokens_for_model(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK: anger\n",
      "[('feel', 2843), ('like', 563), ('im', 444), ('get', 282), ('peopl', 203), ('time', 202), ('want', 196), ('know', 188), ('realli', 186), ('think', 177)]\n",
      "NLTK: fear\n",
      "[('feel', 2843), ('like', 563), ('im', 444), ('get', 282), ('peopl', 203), ('time', 202), ('want', 196), ('know', 188), ('realli', 186), ('think', 177)]\n",
      "NLTK: joy\n",
      "[('feel', 2843), ('like', 563), ('im', 444), ('get', 282), ('peopl', 203), ('time', 202), ('want', 196), ('know', 188), ('realli', 186), ('think', 177)]\n",
      "NLTK: sadness\n",
      "[('feel', 2843), ('like', 563), ('im', 444), ('get', 282), ('peopl', 203), ('time', 202), ('want', 196), ('know', 188), ('realli', 186), ('think', 177)]\n",
      "NLTK: love\n",
      "[('feel', 2843), ('like', 563), ('im', 444), ('get', 282), ('peopl', 203), ('time', 202), ('want', 196), ('know', 188), ('realli', 186), ('think', 177)]\n",
      "NLTK: surprise\n",
      "[('feel', 2843), ('like', 563), ('im', 444), ('get', 282), ('peopl', 203), ('time', 202), ('want', 196), ('know', 188), ('realli', 186), ('think', 177)]\n",
      "NLTK: corporate\n",
      "[('feel', 2843), ('like', 563), ('im', 444), ('get', 282), ('peopl', 203), ('time', 202), ('want', 196), ('know', 188), ('realli', 186), ('think', 177)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Barclays CEO stresses the importance of regulatory and cultural reform in financial services at Brussels conference  http://t.co/Ge9Lp7hpyG',\n",
       "       'Barclays announces result of Rights Issue http://t.co/LbIqqh3wwG',\n",
       "       'Barclays publishes its prospectus for its å£5.8bn Rights Issue: http://t.co/YZk24iE8G6',\n",
       "       ...,\n",
       "       'Yesterday, these #HealthyKids lit up Broadway with #Nestle, @iaaforg and some sporting stars: http://t.co/YdtBj60Ofz',\n",
       "       'Yo-Jelly, Danone new brand in South Africa : the fun taste sensation of jelly &amp; the health benefits of yoghurt ! #Danone #Yojelly',\n",
       "       'Z Bhutta: Problems with food&amp;land systems include land acquistion, commodity speculation affecting food prices&amp;lack of discussion #NINS2013'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions = df['emotion'].drop_duplicates().tolist()\n",
    "global_dataset = {}\n",
    "\n",
    "for em in emotions:\n",
    "    print(\"NLTK:\", em)\n",
    "    dataset = df[df['emotion'] == em]['text'].astype('str').to_numpy()\n",
    "    \n",
    "    tokens = [nltk.word_tokenize(text) for text in dataset]\n",
    "    cleaned_tokens = [remove_noise(token, stopwords) for token in tokens]\n",
    "    words = get_all_words(cleaned_tokens)\n",
    "    \n",
    "    freq_dist = FreqDist(words)\n",
    "    print(freq_dist_pos.most_common(10))\n",
    "    \n",
    "    global_dataset[em] = [(text_dict, em) for text_dict in get_tokens_for_model(cleaned_tokens)]\n",
    "\n",
    "global_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27078"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_for_model = []\n",
    "for key in global_dataset.keys():\n",
    "    dataset_for_model += global_dataset[key]\n",
    "random.shuffle(dataset_for_model)\n",
    "len(dataset_for_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: 21662\n",
      "test data: 5416\n"
     ]
    }
   ],
   "source": [
    "text_count = (df.shape[0] * 80) // 100\n",
    "\n",
    "train_data = dataset_for_model[:text_count]\n",
    "test_data = dataset_for_model[text_count:]\n",
    "\n",
    "print(\"train data:\", len(train_data))\n",
    "print(\"test data:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.7324593796159528\n",
      "Most Informative Features\n",
      "                    feel = True           surpri : corpor =    550.8 : 1.0\n",
      "                    feel = None           corpor : love   =    526.2 : 1.0\n",
      "                    http = True           corpor : fear   =    395.1 : 1.0\n",
      "                   nestl = True           corpor : sadnes =    329.1 : 1.0\n",
      "                  health = True           corpor : fear   =    215.8 : 1.0\n",
      "                      rt = True           corpor : sadnes =    215.3 : 1.0\n",
      "                       1 = True           corpor : joy    =    193.1 : 1.0\n",
      "                    daze = True           surpri : joy    =    188.3 : 1.0\n",
      "                 curious = True           surpri : anger  =    185.8 : 1.0\n",
      "               overwhelm = True           surpri : corpor =    173.7 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct answers: 73.24593796159527 %\n"
     ]
    }
   ],
   "source": [
    "result = [classifier.classify(test_data[i][0]) == test_data[i][1] for i in range(len(test_data))]\n",
    "print(\"Correct answers:\", sum(result) * 100 / len(result), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(text):\n",
    "    tokens = remove_noise(nltk.tokenize.word_tokenize(text))\n",
    "    return classifier.classify(dict([token, True] for token in tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'corporate'"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(\"we prefer to work with kanban, so please answer me asap\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('dl': conda)",
   "language": "python",
   "name": "python38264bitdlcondaa180a010ef684b4caaeb2a04b88c216b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
